# This is the NLP Group GitHub Repository. 

You will be able to find papers we have discussed, useful links and resources and a summary table of benefits and drawbacks there are to using some of these methodologies here.  

# Papers

Prior to a paper presentation please add your paper in the `papers` folder, naming the PDF after its title ( <title.pdf> ) so we have a copy and make sure we do not present the same paper twice.

Don't forget to check the [Wiki](https://github.com/datasciencecampus/nlp_club/wiki) for videos and references.

# Past Papers

| Date | Title of paper | Source |
|------|----------------|--------|
|2019-01-09|Enriching Word Vectors with Subword Information | [aclweb](http://aclweb.org/anthology/D18-1482) |
|2019-01-23|Word Moverâ€™s Embedding: From Word2Vec to Document Embedding| [aclweb](http://aclweb.org/anthology/D18-1482) |
|2019-02-06|Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank| [Stanford University](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)|
|2019-02-20|Deep contextualized word representations|[arxiv](https://arxiv.org/pdf/1802.05365.pdf)|
|2019-03-06|Efficient Estimation of Word Representations in Vector Space|[arxiv](https://arxiv.org/pdf/1301.3781.pdf)|
|2019-03-20|Distributed Representations of Words and Phrases and their Compositionality|[arxiv](https://arxiv.org/abs/1310.4546)|
|2019-06-26|Distributed Representations of Sentences and Documents|[Stanford University](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) |
|2019-07-24|Latent Dirichlet Allocation|[Stanford](https://ai.stanford.edu/~ang/papers/nips01-lda.pdf)|
|2019-07-31|Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec|[arxiv](https://arxiv.org/abs/1605.02019)|
|2019-09-18|GloVe: Global Vectors for Word Representation| [Stanford University](https://nlp.stanford.edu/pubs/glove.pdf)|
|2019-10-03|Algorithms for Non-negative Matrix Factorization| [paper](https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf)|
|2019-10-23| Effective Approaches to Attention-based Neural Machine Translation| [arxiv](https://arxiv.org/pdf/1508.04025.pdf)|
|2020-02-12| Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks| [arxiv](https://arxiv.org/abs/1908.10084)|
|2020-02-26| XLNet: Generalized Autoregressive Pretraining for Language Understanding| [arxiv](https://arxiv.org/pdf/1906.08237.pdf))|

# Future papers 
- **CNN** Understanding Convolutional Neural Networks for Text Classification (CNN paper; source: [aclweb](https://www.aclweb.org/anthology/W18-5408))
- **LSTM** An LSTM Approach to Short Text Sentiment Classification with
Word Embeddings (LSTM paper; source: [aclweb](https://www.aclweb.org/anthology/O18-1021))
- **eLmo (revisit)** Deep contextualized word representations (ELMo paper; source: [arxiv](https://arxiv.org/pdf/1802.05365.pdf))
- **Bert** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Bert paper; source: [arxiv](https://arxiv.org/abs/1810.04805))
- **Severn** 
- [On the dimensionality of Word Embeddings](https://arxiv.org/abs/1812.04224) (how many dimensions are required?)
- [Visualizing Data using t-SNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) (source: Journal Machine Learning Research)
- **Google Universal Sentence Embedding** (Universal Sentence encoder; source: [Google](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf)) 
